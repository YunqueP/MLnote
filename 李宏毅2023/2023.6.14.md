# 2023.6.14

## 1. 机器学习是什么？

概括的来说，机器学习就是让机器学会自动找一个函式的能力

## 2.机器学习的任务是什么

- Regression 回归问题 输出的是一个数值，比如PM2.5的预测

- Classification 分类问题 输出的是离散值，比如二分类的输出就是0或1，是或否；多分类的输出就是[1,2,...,N]，比如判断一张图片是猫还是狗还是一双鞋

- Structured learning 结构学习，输出的是有结构的内容，比如影像、文句

## 3.机器学习的步骤

- （1）设定范围：定出**候选函式**(model)的集合，Function with unknown parameters（写出一个带有未知参数的函数，比如deep learning中的RNN， 比如decision tree

- （2）设定标准：定出“评量函式好坏”的标准，define loss function from training data（定义loss函数）

- （3）达成目标：找出最好的函式optimization，方法有Gradient decent、Genetic Algorithm等

### 举例

机器学习找函式的三个步骤，我们用b站视频播放量预测这个问题，来简单说明是怎么运作的。

### Step 1.Function with unknown parameters

第一个步骤是我们需要写出一个带有未知参数的函式，即候选函式的集合。举例，我们先做一个初步的猜测，假设函式为$y=ax+b$

**y是我们准备预测的变量，即播放量**

**x是这个视频前一天的观看人数，y和x都是数值，称作特征**

**a和b是未知的参数，这是需要我们通过训练资料找出来的**



显然，用线性函数作为候选函式不一定准确，在设定范围的步骤，要根据我们对一个问题本质上的了解，确定合适的候选集合。我们的猜测不一定准确，还有可能需要修正。



### Step 2.Define loss from training data

定义Loss函数，这个函数的输入是参数，即Model里面的a和b；输出的值代表这笔参数好还是不好。那么这个Loss是如何计算的呢？



函式预估的结果跟训练数据里真正的结果（这个真实的值叫Label）的差距有多大。估计的值用$y$来表示，真实的值用$\widehat{y}$来表示，两者之间的差为$e$

**计算差距的方法不只有一种**

有绝对误差(MAE)、平方误差(MSE)、交叉熵(Cross-entropy)等等，具体如何计算差距，根据具体情况具体分析



把训练数据得到的每一天的误差加起来求平均，算出一个L，就是Loss。L越大，代表现在这一组参数越不好，L越小，代表现在这一组参数越好。把每一组参数得到的Loss做成等高线图，我们就得到了**Error Surface**。这就是机器学习的第二步。



### Step 3.Optimization

解一个最优化问题。找到一个a和b，使得L的值最小。此时的a和b就是我们需要的参数。

这里我们可以使用梯度下降法(Gradient Descent)来实现。



## 4.需要注意的地方

讲完了机器学习的三个步骤，接下来我们从最后一个步骤逆推回去，看看我们需要注意些什么。

- 达成目标

![最优化算法注意点.png](https://cdn.jsdelivr.net/gh/YunqueP/PictureHere@main/img/%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%B3%A8%E6%84%8F%E7%82%B9.png)

在达成目标阶段，最重要的就是我们的Optimization Algorithm，我们需要先设定一些**超参数**(Hyperparameter)来更好的最优化。包括learning rate,batch size,how to init等等，我们常说的机器学习的调参数，大部分情况下都是在调整超参数。



- 设定标准

训练数据中表现良好的参数，放到测试数据中，表现就一定良好吗？答案是否定的。



因为训练数据和测试数据的分布不一定是相同的。机器学习的标准定在训练数据，而用在测试数据。模型在训练集上表现很好，到了验证和测试阶段，表现就很差，出现了**过拟合**现象，表明模型的泛化能力很差。



我们常常在Loss上做额外的考量（如regularization），来尽量避免过拟合现象的发生。



- 设定范围

在选定候选函式的集合时，我们要充分考量任务的特性。

**需要特别注意，不是Loss越小，就说明我们的函式集合选的越好**

以学习做例子，如果死记硬背，那么我们在复习的时候（相当于训练集）会表现得很好，即Loss很低，但是在考试的时候（相当于测试集）不一定会表现得好，甚至会很差。



训练数据少的时候，范围选择比较保守；训练数据多的时候，范围可以扩大



**候选函式的范围不可太小也不可太大，要在合适的范围内**

所以说，设定范围也是一门很有技巧性的学问。
